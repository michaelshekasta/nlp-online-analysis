{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from bidi.algorithm import get_display\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smtplib\n",
    "from os.path import basename\n",
    "from email.mime.application import MIMEApplication\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "from email.utils import COMMASPACE, formatdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully sent the mail\n"
     ]
    }
   ],
   "source": [
    "send_email()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_email(user='dsakaidf@gmail.com', pwd='d54k4idf!', recipient='shkasta@post.bgu.ac.il',\n",
    "               subject='finish expirement', body='my password is d54k4idf!'):\n",
    "    import smtplib\n",
    "\n",
    "    gmail_user = user\n",
    "    gmail_pwd = pwd\n",
    "    FROM = user\n",
    "    TO = recipient if type(recipient) is list else [recipient]\n",
    "    SUBJECT = subject\n",
    "    TEXT = body\n",
    "\n",
    "    # Prepare actual message\n",
    "    message = \"\"\"From: %s\\nTo: %s\\nSubject: %s\\n\\n%s\n",
    "    \"\"\" % (FROM, \", \".join(TO), SUBJECT, TEXT)\n",
    "    try:\n",
    "        # SMTP_SSL Example\n",
    "        server_ssl = smtplib.SMTP_SSL(\"smtp.gmail.com\", 465)\n",
    "        server_ssl.ehlo()  # optional, called by login()\n",
    "        server_ssl.login(gmail_user, gmail_pwd)\n",
    "        # ssl server doesn't support or need tls, so don't call server_ssl.starttls()\n",
    "        server_ssl.sendmail(FROM, TO, message)\n",
    "        # server_ssl.quit()\n",
    "        server_ssl.close()\n",
    "        print('successfully sent the mail')\n",
    "    except:\n",
    "        print(\"failed to send mail\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "he_nlp = stanza.Pipeline('he', processors='tokenize,mwt', verbose=False, use_gpu=False)\n",
    "#he_nlp = stanza.Pipeline('he', processors='tokenize,lemma,pos', verbose=False, use_gpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.0.0.json: 116kB [00:00, 1.10MB/s]\n",
      "2020-04-28 23:41:17 INFO: Downloading default packages for language: he (Hebrew)...\n",
      "2020-04-28 23:41:17 INFO: File exists: C:\\Users\\Michael\\stanza_resources\\he\\default.zip.\n",
      "2020-04-28 23:41:20 INFO: Finished downloading models and saved to C:\\Users\\Michael\\stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "stanza.download('he', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_file = 'post'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_name = 'message'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = os.listdir('data/')\n",
    "for filename in file_name:\n",
    "    if type_file in filename:\n",
    "        temp_df = pd.read_csv(f'data/{filename}')\n",
    "        if threads_df is None:\n",
    "            threads_df = temp_df\n",
    "        else:\n",
    "            threads_df = threads_df.append(temp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = threads_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>thread_id</th>\n",
       "      <th>post_id</th>\n",
       "      <th>user_name</th>\n",
       "      <th>date</th>\n",
       "      <th>message</th>\n",
       "      <th>cite1</th>\n",
       "      <th>cite2</th>\n",
       "      <th>cite3</th>\n",
       "      <th>cite4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>20126916</td>\n",
       "      <td>post_202834374</td>\n",
       "      <td>×¨×•××</td>\n",
       "      <td>21-02-2020Â 20:54</td>\n",
       "      <td>\\n×™×© ×›××œ×” ×‘×›×œ×œ?\\n \\n \\n \\n ×˜×™×¡ ×¢×“ ×œ×’×™×‘×•×©Â  \\n×©×—...</td>\n",
       "      <td>login.php?do=lostpw</td>\n",
       "      <td>login.php?do=lostpw</td>\n",
       "      <td>login.php?do=lostpw</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20126916</td>\n",
       "      <td>post_202834407</td>\n",
       "      <td>×¨×•××</td>\n",
       "      <td>21-02-2020Â 20:54</td>\n",
       "      <td>\\n×™×© ×›××œ×” ×‘×›×œ×œ?\\n \\n \\n \\n ×˜×™×¡ ×¢×“ ×œ×’×™×‘×•×©Â  \\n×©×—...</td>\n",
       "      <td>login.php?do=lostpw</td>\n",
       "      <td>login.php?do=lostpw</td>\n",
       "      <td>login.php?do=lostpw</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>20126916</td>\n",
       "      <td>post_202834485</td>\n",
       "      <td>×¨×•××</td>\n",
       "      <td>21-02-2020Â 20:54</td>\n",
       "      <td>\\n×™×© ×›××œ×” ×‘×›×œ×œ?\\n \\n \\n \\n ×˜×™×¡ ×¢×“ ×œ×’×™×‘×•×©Â  \\n×©×—...</td>\n",
       "      <td>login.php?do=lostpw</td>\n",
       "      <td>login.php?do=lostpw</td>\n",
       "      <td>login.php?do=lostpw</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>20126916</td>\n",
       "      <td>post_202834494</td>\n",
       "      <td>×¨×•××</td>\n",
       "      <td>21-02-2020Â 20:54</td>\n",
       "      <td>\\n×™×© ×›××œ×” ×‘×›×œ×œ?\\n \\n \\n \\n ×˜×™×¡ ×¢×“ ×œ×’×™×‘×•×©Â  \\n×©×—...</td>\n",
       "      <td>login.php?do=lostpw</td>\n",
       "      <td>login.php?do=lostpw</td>\n",
       "      <td>login.php?do=lostpw</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>20126916</td>\n",
       "      <td>post_202834534</td>\n",
       "      <td>×¨×•××</td>\n",
       "      <td>21-02-2020Â 20:54</td>\n",
       "      <td>\\n×™×© ×›××œ×” ×‘×›×œ×œ?\\n \\n \\n \\n ×˜×™×¡ ×¢×“ ×œ×’×™×‘×•×©Â  \\n×©×—...</td>\n",
       "      <td>login.php?do=lostpw</td>\n",
       "      <td>login.php?do=lostpw</td>\n",
       "      <td>login.php?do=lostpw</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  thread_id         post_id user_name              date  \\\n",
       "0           0   20126916  post_202834374      ×¨×•××  21-02-2020Â 20:54   \n",
       "1           1   20126916  post_202834407      ×¨×•××  21-02-2020Â 20:54   \n",
       "2           2   20126916  post_202834485      ×¨×•××  21-02-2020Â 20:54   \n",
       "3           3   20126916  post_202834494      ×¨×•××  21-02-2020Â 20:54   \n",
       "4           4   20126916  post_202834534      ×¨×•××  21-02-2020Â 20:54   \n",
       "\n",
       "                                             message                cite1  \\\n",
       "0  \\n×™×© ×›××œ×” ×‘×›×œ×œ?\\n \\n \\n \\n ×˜×™×¡ ×¢×“ ×œ×’×™×‘×•×©Â  \\n×©×—...  login.php?do=lostpw   \n",
       "1  \\n×™×© ×›××œ×” ×‘×›×œ×œ?\\n \\n \\n \\n ×˜×™×¡ ×¢×“ ×œ×’×™×‘×•×©Â  \\n×©×—...  login.php?do=lostpw   \n",
       "2  \\n×™×© ×›××œ×” ×‘×›×œ×œ?\\n \\n \\n \\n ×˜×™×¡ ×¢×“ ×œ×’×™×‘×•×©Â  \\n×©×—...  login.php?do=lostpw   \n",
       "3  \\n×™×© ×›××œ×” ×‘×›×œ×œ?\\n \\n \\n \\n ×˜×™×¡ ×¢×“ ×œ×’×™×‘×•×©Â  \\n×©×—...  login.php?do=lostpw   \n",
       "4  \\n×™×© ×›××œ×” ×‘×›×œ×œ?\\n \\n \\n \\n ×˜×™×¡ ×¢×“ ×œ×’×™×‘×•×©Â  \\n×©×—...  login.php?do=lostpw   \n",
       "\n",
       "                 cite2                cite3 cite4  \n",
       "0  login.php?do=lostpw  login.php?do=lostpw   NaN  \n",
       "1  login.php?do=lostpw  login.php?do=lostpw   NaN  \n",
       "2  login.php?do=lostpw  login.php?do=lostpw   NaN  \n",
       "3  login.php?do=lostpw  login.php?do=lostpw   NaN  \n",
       "4  login.php?do=lostpw  login.php?do=lostpw   NaN  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n×™×© ×›××œ×” ×‘×›×œ×œ?\\n \\n \\n \\n ×˜×™×¡ ×¢×“ ×œ×’×™×‘×•×©Â  \\n×©×—...</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n×™×© ×›××œ×” ×‘×›×œ×œ?\\n \\n \\n \\n ×˜×™×¡ ×¢×“ ×œ×’×™×‘×•×©Â  \\n×©×—...</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n×™×© ×›××œ×” ×‘×›×œ×œ?\\n \\n \\n \\n ×˜×™×¡ ×¢×“ ×œ×’×™×‘×•×©Â  \\n×©×—...</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n×™×© ×›××œ×” ×‘×›×œ×œ?\\n \\n \\n \\n ×˜×™×¡ ×¢×“ ×œ×’×™×‘×•×©Â  \\n×©×—...</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n×™×© ×›××œ×” ×‘×›×œ×œ?\\n \\n \\n \\n ×˜×™×¡ ×¢×“ ×œ×’×™×‘×•×©Â  \\n×©×—...</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  word_count\n",
       "0  \\n×™×© ×›××œ×” ×‘×›×œ×œ?\\n \\n \\n \\n ×˜×™×¡ ×¢×“ ×œ×’×™×‘×•×©Â  \\n×©×—...          35\n",
       "1  \\n×™×© ×›××œ×” ×‘×›×œ×œ?\\n \\n \\n \\n ×˜×™×¡ ×¢×“ ×œ×’×™×‘×•×©Â  \\n×©×—...          35\n",
       "2  \\n×™×© ×›××œ×” ×‘×›×œ×œ?\\n \\n \\n \\n ×˜×™×¡ ×¢×“ ×œ×’×™×‘×•×©Â  \\n×©×—...          35\n",
       "3  \\n×™×© ×›××œ×” ×‘×›×œ×œ?\\n \\n \\n \\n ×˜×™×¡ ×¢×“ ×œ×’×™×‘×•×©Â  \\n×©×—...          35\n",
       "4  \\n×™×© ×›××œ×” ×‘×›×œ×œ?\\n \\n \\n \\n ×˜×™×¡ ×¢×“ ×œ×’×™×‘×•×©Â  \\n×©×—...          35"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fetch wordcount for each abstract\n",
    "dataset['word_count'] = dataset[field_name].apply(lambda x: len(str(x).split(\" \")))\n",
    "dataset[[field_name,'word_count']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    334819.000000\n",
       "mean        224.657269\n",
       "std         177.378942\n",
       "min           1.000000\n",
       "25%         110.000000\n",
       "50%         186.000000\n",
       "75%         290.000000\n",
       "max        5242.000000\n",
       "Name: word_count, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.word_count.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "×œ×     1935550\n",
       "×–×”     1462097\n",
       "×× ×™    1037347\n",
       "××ª      939085\n",
       "××ª×”     778459\n",
       "×¢×œ      706923\n",
       "××”      686145\n",
       "××      668095\n",
       "×œ×™      657111\n",
       "××‘×œ     585003\n",
       "×™×©      560402\n",
       "×œ×š      558872\n",
       "×©×œ      548140\n",
       "×¢×      464913\n",
       "××•      434318\n",
       "×’×      405743\n",
       "×›×œ      381187\n",
       "××–      369157\n",
       "×”×•×     299469\n",
       "×›×™      297944\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = pd.Series(' '.join(dataset[field_name]).split()).value_counts()[:20]\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Creating a list of stop words and adding custom stopwords\n",
    "import codecs\n",
    "with codecs.open('stopwords.txt', 'r', encoding='utf8') as f:\n",
    "    text = f.read()\n",
    "stop_words = set(word for word in text.split('\\r\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\aten\\src\\ATen\\native\\BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤­ğŸ™…*â™‚\n",
      "ğŸ¤­ğŸ™…*â™‚\n",
      "ğŸ¤­ğŸ™…*â™‚\n",
      "ğŸ¤­ğŸ™…*â™‚\n",
      "ğŸ¤­ğŸ™…*â™‚\n",
      "ğŸ¤­ğŸ™…*â™‚\n",
      "ğŸ¤­ğŸ™…*â™‚\n",
      "ğŸ¤­ğŸ™…*â™‚\n",
      "ğŸ¤­ğŸ™…*â™‚\n",
      "ğŸ¤­ğŸ™…*â™‚\n",
      "ğŸ¤­ğŸ™…*â™‚\n",
      "ğŸ¤­ğŸ™…*â™‚\n",
      "ğŸ¤­ğŸ™…*â™‚\n",
      "ğŸ¤­ğŸ™…*â™‚\n",
      "ğŸ¤­ğŸ™…*â™‚\n",
      "â¦â¤ï¸â©\n",
      "â¦â¤ï¸â©\n",
      "â¦â¤ï¸â©\n",
      "â¦â¤ï¸â©\n",
      "â¦â¤ï¸â©\n",
      "â¦â¤ï¸â©\n",
      "â¦â¤ï¸â©\n",
      "â¦â¤ï¸â©\n",
      "â¦â¤ï¸â©\n",
      "â¦â¤ï¸â©\n",
      "â¦â¤ï¸â©\n",
      "â¦â¤ï¸â©\n",
      "â¦â¤ï¸â©\n",
      "â¦â¤ï¸â©\n",
      "â¦â¤ï¸â©\n",
      "ğŸ¤¨×–×”\n",
      "ğŸ¤¨×–×”\n",
      "ğŸ¤¨×–×”\n",
      "ğŸ¤¨×–×”\n",
      "ğŸ¤¨×–×”\n",
      "ğŸ¤¨×–×”\n",
      "ğŸ¤¨×–×”\n",
      "ğŸ¤¨×–×”\n",
      "ğŸ¤¨×–×”\n",
      "ğŸ¤¨×–×”\n",
      "ğŸ¤¨×–×”\n",
      "ğŸ¤¨×–×”\n",
      "ğŸ¤¨×–×”\n",
      "ğŸ¤¨×–×”\n",
      "ğŸ¤¨×–×”\n",
      "××—×©×œ×™×™×™×™×™×™×™×™×™×™×™×™×™â¦âœŒï¸â©\n",
      "××—×©×œ×™×™×™×™×™×™×™×™×™×™×™×™×™â¦âœŒï¸â©\n",
      "××—×©×œ×™×™×™×™×™×™×™×™×™×™×™×™×™â¦âœŒï¸â©\n",
      "××—×©×œ×™×™×™×™×™×™×™×™×™×™×™×™×™â¦âœŒï¸â©\n",
      "××—×©×œ×™×™×™×™×™×™×™×™×™×™×™×™×™â¦âœŒï¸â©\n",
      "××—×©×œ×™×™×™×™×™×™×™×™×™×™×™×™×™â¦âœŒï¸â©\n",
      "××—×©×œ×™×™×™×™×™×™×™×™×™×™×™×™×™â¦âœŒï¸â©\n",
      "××—×©×œ×™×™×™×™×™×™×™×™×™×™×™×™×™â¦âœŒï¸â©\n",
      "××—×©×œ×™×™×™×™×™×™×™×™×™×™×™×™×™â¦âœŒï¸â©\n",
      "××—×©×œ×™×™×™×™×™×™×™×™×™×™×™×™×™â¦âœŒï¸â©\n",
      "××—×©×œ×™×™×™×™×™×™×™×™×™×™×™×™×™â¦âœŒï¸â©\n",
      "××—×©×œ×™×™×™×™×™×™×™×™×™×™×™×™×™â¦âœŒï¸â©\n",
      "××—×©×œ×™×™×™×™×™×™×™×™×™×™×™×™×™â¦âœŒï¸â©\n",
      "××—×©×œ×™×™×™×™×™×™×™×™×™×™×™×™×™â¦âœŒï¸â©\n",
      "××—×©×œ×™×™×™×™×™×™×™×™×™×™×™×™×™â¦âœŒï¸â©\n",
      "ğŸ¤¨×©×§×˜\n",
      "ğŸ¤¨×©×§×˜\n",
      "ğŸ¤¨×©×§×˜\n",
      "ğŸ¤¨×©×§×˜\n",
      "ğŸ¤¨×©×§×˜\n",
      "ğŸ¤¨×©×§×˜\n",
      "ğŸ¤¨×©×§×˜\n",
      "ğŸ¤¨×©×§×˜\n",
      "ğŸ¤¨×©×§×˜\n",
      "ğŸ¤¨×©×§×˜\n",
      "ğŸ¤¨×©×§×˜\n",
      "ğŸ¤¨×©×§×˜\n",
      "ğŸ‘¨*ğŸ¦°\n",
      "ğŸ¦¹*â™‚\n",
      "ğŸ‘¨*ğŸ¦°\n",
      "ğŸ‘¨*ğŸ¦°\n",
      "ğŸ¦¹*â™‚\n",
      "ğŸ‘¨*ğŸ¦°\n",
      "ğŸ‘¨*ğŸ¦°\n",
      "ğŸ¦¹*â™‚\n",
      "ğŸ‘¨*ğŸ¦°\n",
      "ğŸ‘¨*ğŸ¦°\n",
      "ğŸ¦¹*â™‚\n",
      "ğŸ‘¨*ğŸ¦°\n",
      "ğŸ‘¨*ğŸ¦°\n",
      "ğŸ¦¹*â™‚\n",
      "ğŸ‘¨*ğŸ¦°\n",
      "ğŸ‘¨*ğŸ¦°\n",
      "ğŸ¦¹*â™‚\n",
      "ğŸ‘¨*ğŸ¦°\n",
      "ğŸ‘¨*ğŸ¦°\n",
      "ğŸ¦¹*â™‚\n",
      "ğŸ‘¨*ğŸ¦°\n",
      "ğŸ‘¨*ğŸ¦°\n",
      "ğŸ¦¹*â™‚\n",
      "ğŸ‘¨*ğŸ¦°\n",
      "ğŸ‘¨*ğŸ¦°\n",
      "ğŸ¦¹*â™‚\n",
      "ğŸ‘¨*ğŸ¦°\n",
      "ğŸ‘¨*ğŸ¦°\n",
      "ğŸ¦¹*â™‚\n",
      "ğŸ‘¨*ğŸ¦°\n",
      "ğŸ‘¨*ğŸ¦°\n",
      "ğŸ¦¹*â™‚\n",
      "ğŸ‘¨*ğŸ¦°\n",
      "ğŸ‘¨*ğŸ¦°\n",
      "ğŸ¦¹*â™‚\n",
      "ğŸ‘¨*ğŸ¦°\n",
      "ğŸ‘¨*ğŸ¦°\n",
      "ğŸ¦¹*â™‚\n",
      "ğŸ‘¨*ğŸ¦°\n",
      "ğŸ‘¨*ğŸ¦°\n",
      "ğŸ¦¹*â™‚\n",
      "ğŸ‘¨*ğŸ¦°\n",
      "ğŸ‘¨*ğŸ¦°\n",
      "ğŸ¦¹*â™‚\n",
      "ğŸ‘¨*ğŸ¦°\n",
      "×™×•×¨×“ğŸ˜„×œ×™ğŸ¥¯×¢×œ\n",
      "×™×•×¨×“ğŸ˜„×œ×™ğŸ¥¯×¢×œ\n",
      "×™×•×¨×“ğŸ˜„×œ×™ğŸ¥¯×¢×œ\n",
      "×™×•×¨×“ğŸ˜„×œ×™ğŸ¥¯×¢×œ\n",
      "×™×•×¨×“ğŸ˜„×œ×™ğŸ¥¯×¢×œ\n",
      "×™×•×¨×“ğŸ˜„×œ×™ğŸ¥¯×¢×œ\n",
      "×™×•×¨×“ğŸ˜„×œ×™ğŸ¥¯×¢×œ\n",
      "×™×•×¨×“ğŸ˜„×œ×™ğŸ¥¯×¢×œ\n",
      "×™×•×¨×“ğŸ˜„×œ×™ğŸ¥¯×¢×œ\n",
      "ğŸ‘¶ğŸ»â©\n",
      "ğŸ‘¶ğŸ»â©\n",
      "ğŸ‘¶ğŸ»â©\n",
      "ğŸ‘¶ğŸ»â©\n",
      "ğŸ‘¶ğŸ»â©\n",
      "ğŸ‘¶ğŸ»â©\n",
      "ğŸ‘¶ğŸ»â©\n",
      "ğŸ‘¶ğŸ»â©\n",
      "ğŸ‘¶ğŸ»â©\n",
      "ğŸ‘¶ğŸ»â©\n",
      "ğŸ‘¶ğŸ»â©\n",
      "ğŸ‘¶ğŸ»â©\n",
      "ğŸ‘¶ğŸ»â©\n",
      "ğŸ‘¶ğŸ»â©\n",
      "ğŸ‘¶ğŸ»â©\n",
      "â¦â¤ï¸â©\n",
      "â¦â¤ï¸â©\n",
      "â¦â¤ï¸â©\n",
      "â¦â¤ï¸â©\n",
      "â¦â¤ï¸â©\n",
      "â¦â¤ï¸â©\n",
      "â¦â¤ï¸â©\n",
      "â¦â¤ï¸â©\n",
      "â¦â¤ï¸â©\n",
      "â¦â¤ï¸â©\n",
      "â¦â¤ï¸â©\n",
      "ğŸ¥°ğŸ¥°\n",
      "ğŸ¥°ğŸ¥°\n",
      "ğŸ¥°ğŸ¥°\n",
      "ğŸ¥°ğŸ¥°\n",
      "ğŸ¥°ğŸ¥°\n",
      "ğŸ¥°ğŸ¥°\n",
      "ğŸ¥°ğŸ¥°\n",
      "ğŸ¥°ğŸ¥°\n",
      "ğŸ¥°ğŸ¥°\n",
      "ğŸ¥°ğŸ¥°\n",
      "ğŸ¥°ğŸ¥°\n",
      "ğŸ¥°ğŸ¥°\n",
      "ğŸ¥°ğŸ¥°\n",
      "ğŸ¥°ğŸ¥°\n",
      "ğŸ¥°ğŸ¥°\n",
      "×¨×•×× ×˜×™â¦\n",
      "×¨×•×× ×˜×™â¦\n",
      "×¨×•×× ×˜×™â¦\n",
      "×¨×•×× ×˜×™â¦\n",
      "×¨×•×× ×˜×™â¦\n",
      "â¦â¤ï¸â©\n",
      "â¦â¤ï¸â©\n",
      "â¦â¤ï¸â©\n",
      "â¦â¤ï¸â©\n",
      "â¦â¤ï¸â©\n",
      "â¦â¤ï¸â©\n",
      "â¦â¤ï¸â©\n",
      "â¦â¤ï¸â©\n",
      "â¦â¤ï¸â©\n",
      "â¦â¤ï¸â©\n",
      "â¦â¤ï¸â©\n",
      "â¦â¤ï¸â©\n",
      "â¦â¤ï¸â©\n",
      "â¦â¤ï¸â©\n",
      "â¦â¤ï¸â©\n",
      "ğŸ¥°ğŸ¥°\n",
      "ğŸ¥°ğŸ¥°\n",
      "ğŸ¥°ğŸ¥°\n",
      "ğŸ¥°ğŸ¥°\n",
      "ğŸ¥°ğŸ¥°\n",
      "ğŸ¥°ğŸ¥°\n",
      "ğŸ¥°ğŸ¥°\n",
      "ğŸ¥°ğŸ¥°\n",
      "ğŸ¥°ğŸ¥°\n",
      "ğŸ¥°ğŸ¥°\n",
      "ğŸ¥°ğŸ¥°\n",
      "ğŸ¥°ğŸ¥°\n",
      "ğŸ¥°ğŸ¥°\n",
      "ğŸ¥°ğŸ¥°\n",
      "ğŸ¥°ğŸ¥°\n",
      "ğŸ¤«ğŸ¤«ğŸ¤«ğŸ¤«ğŸ¤«\n",
      "ğŸ¤«ğŸ¤«ğŸ¤«ğŸ¤«ğŸ¤«\n",
      "ğŸ¤«ğŸ¤«ğŸ¤«ğŸ¤«ğŸ¤«\n",
      "ğŸ¤«ğŸ¤«ğŸ¤«ğŸ¤«ğŸ¤«\n",
      "ğŸ¤«ğŸ¤«ğŸ¤«ğŸ¤«ğŸ¤«\n",
      "ğŸ¤«ğŸ¤«ğŸ¤«ğŸ¤«ğŸ¤«\n",
      "ğŸ¤«ğŸ¤«ğŸ¤«ğŸ¤«ğŸ¤«\n",
      "ğŸ¤«ğŸ¤«ğŸ¤«ğŸ¤«ğŸ¤«\n",
      "ğŸ¤«ğŸ¤«ğŸ¤«ğŸ¤«ğŸ¤«\n",
      "ğŸ¤«ğŸ¤«ğŸ¤«ğŸ¤«ğŸ¤«\n",
      "ğŸ¤«ğŸ¤«ğŸ¤«ğŸ¤«ğŸ¤«\n",
      "ğŸ¤«ğŸ¤«ğŸ¤«ğŸ¤«ğŸ¤«\n",
      "ğŸ¤«ğŸ¤«ğŸ¤«ğŸ¤«ğŸ¤«\n",
      "ğŸ¤«ğŸ¤«ğŸ¤«ğŸ¤«ğŸ¤«\n",
      "ğŸ¤«ğŸ¤«ğŸ¤«ğŸ¤«ğŸ¤«\n",
      "ğŸ¥ºğŸ¥º\n",
      "ğŸ¥ºğŸ¥º\n",
      "ğŸ¥ºğŸ¥º\n",
      "ğŸ¥ºğŸ¥º\n",
      "â¦â™¥ï¸â©â¦â™¥\n",
      "â¦â™¥ï¸â©â¦â™¥\n",
      "â¦â™¥ï¸â©â¦â™¥\n",
      "â¦â™¥ï¸â©â¦â™¥\n",
      "â¦â™¥ï¸â©â¦â™¥\n",
      "â¦â™¥ï¸â©â¦â™¥\n",
      "â¦â™¥ï¸â©â¦â™¥\n",
      "â¦â™¥ï¸â©â¦â™¥\n",
      "â¦â™¥ï¸â©â¦â™¥\n",
      "â¦â™¥ï¸â©â¦â™¥\n",
      "â¦â™¥ï¸â©â¦â™¥\n",
      "ğŸ‘¶ğŸ»â©\n",
      "ğŸ‘¶ğŸ»â©\n",
      "ğŸ‘¶ğŸ»â©\n",
      "ğŸ‘¶ğŸ»â©\n",
      "ğŸ‘¶ğŸ»â©\n",
      "ğŸ‘¶ğŸ»â©\n",
      "ğŸ‘¶ğŸ»â©\n",
      "ğŸ‘¶ğŸ»â©\n",
      "ğŸ‘¶ğŸ»â©\n",
      "ğŸ‘¶ğŸ»â©\n",
      "ğŸ‘¶ğŸ»â©\n",
      "ğŸ‘¶ğŸ»â©\n",
      "ğŸ‘¶ğŸ»â©\n",
      "ğŸ‘¶ğŸ»â©\n",
      "ğŸ‘¶ğŸ»â©\n",
      "â¦â¤ï¸â©\n",
      "×’×•×œ× ×™ğŸ¤®\n",
      "×’×•×œ× ×™ğŸ¤®\n",
      "×’×•×œ× ×™ğŸ¤®\n",
      "×’×•×œ× ×™ğŸ¤®\n",
      "×’×•×œ× ×™ğŸ¤®\n",
      "×’×•×œ× ×™ğŸ¤®\n",
      "×’×•×œ× ×™ğŸ¤®\n",
      "×’×•×œ× ×™ğŸ¤®\n",
      "×’×•×œ× ×™ğŸ¤®\n",
      "×’×•×œ× ×™ğŸ¤®\n",
      "×’×•×œ× ×™ğŸ¤®\n",
      "×’×•×œ× ×™ğŸ¤®\n",
      "×’×•×œ× ×™ğŸ¤®\n",
      "×’×•×œ× ×™ğŸ¤®\n",
      "×’×•×œ× ×™ğŸ¤®\n",
      "â¦â¤ï¸â©\n",
      "â¦â¤ï¸â©\n",
      "â¦â¤ï¸â©\n",
      "â¦â¤ï¸â©\n",
      "â¦â¤ï¸â©\n",
      "â¦â¤ï¸â©\n",
      "â¦â¤ï¸â©\n",
      "â¦â¤ï¸â©\n",
      "â¦â¤ï¸â©\n",
      "â¦â¤ï¸â©\n",
      "â¦â¤ï¸â©\n",
      "â¦â¤ï¸â©\n",
      "â¦â¤ï¸â©\n",
      "â¦â¤ï¸â©\n",
      "×”×¤â¦ğŸ‘ğŸ»\n",
      "×”×¤â¦ğŸ‘ğŸ»\n",
      "×”×¤â¦ğŸ‘ğŸ»\n",
      "×”×¤â¦ğŸ‘ğŸ»\n",
      "×”×¤â¦ğŸ‘ğŸ»\n",
      "ğŸ¤¨ğŸ¤¨ğŸ¤¨ğŸ¤¨\n",
      "ğŸ¤¨ğŸ¤¨ğŸ¤¨ğŸ¤¨\n",
      "ğŸ¤¨ğŸ¤¨ğŸ¤¨ğŸ¤¨\n",
      "ğŸ¤¨ğŸ¤¨ğŸ¤¨ğŸ¤¨\n",
      "ğŸ¤¨ğŸ¤¨ğŸ¤¨ğŸ¤¨\n",
      "ğŸ¤¨ğŸ¤¨ğŸ¤¨ğŸ¤¨\n",
      "ğŸ¤¨ğŸ¤¨ğŸ¤¨ğŸ¤¨\n",
      "ğŸ¤¨ğŸ¤¨ğŸ¤¨ğŸ¤¨\n",
      "ğŸ¤¨ğŸ¤¨ğŸ¤¨ğŸ¤¨\n",
      "ğŸ¤¨ğŸ¤¨ğŸ¤¨ğŸ¤¨\n",
      "ğŸ¤¨ğŸ¤¨ğŸ¤¨ğŸ¤¨\n",
      "ğŸ¤¨ğŸ¤¨ğŸ¤¨ğŸ¤¨\n",
      "ğŸ¤¨ğŸ¤¨ğŸ¤¨ğŸ¤¨\n",
      "ğŸ¤¨ğŸ¤¨ğŸ¤¨ğŸ¤¨\n",
      "ğŸ¤¨ğŸ¤¨ğŸ¤¨ğŸ¤¨\n",
      "ğŸ¤¯ğŸ¤”\n",
      "ğŸ¤¯ğŸ¤”\n",
      "ğŸ¤¯ğŸ¤”\n",
      "ğŸ¤¯ğŸ¤”\n",
      "ğŸ¤¯ğŸ¤”\n",
      "ğŸ¤¯ğŸ¤”\n",
      "ğŸ¤¯ğŸ¤”\n",
      "ğŸ¤¯ğŸ¤”\n",
      "ğŸ¤¯ğŸ¤”\n",
      "ğŸ¤¯ğŸ¤”\n",
      "ğŸ¤¯ğŸ¤”\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "count = 0\n",
    "for i in dataset[field_name]:\n",
    "    text = []\n",
    "    try:        \n",
    "        he_doc = he_nlp(re.sub('- FXP', '', i))\n",
    "    except:\n",
    "        count += 1\n",
    "        continue\n",
    "    for i, sent in enumerate(he_doc.sentences):\n",
    "        for word in reversed(sent.words):\n",
    "            word_text = word.text\n",
    "            if not '_' in word_text and not word_text in stop_words and len(word_text) > 1:\n",
    "                try:\n",
    "                    text.append(get_display(word_text))\n",
    "                except:\n",
    "                    print(word.text)\n",
    "    if len(text) > 0:\n",
    "        corpus.append(' '.join(text))\n",
    "print(f'number remove setence {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "wordcloud = WordCloud(\n",
    "                          background_color='white',\n",
    "                          stopwords=stop_words,\n",
    "                          max_words=100,\n",
    "                          max_font_size=50, \n",
    "                          random_state=42,\n",
    "                          font_path='C:/WINDOWS/Fonts/Arial.ttf'\n",
    "                         ).generate(' '.join(corpus))\n",
    "print(wordcloud)\n",
    "fig = plt.figure(1)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "fig.savefig(\"word1.png\", dpi=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_corpus = list()\n",
    "for sentence in corpus:\n",
    "    new_corpus.append(' '.join([word for word in sentence.split() if not word in stop_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = new_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "cv=CountVectorizer(max_df=0.8,stop_words=stop_words, max_features=10000, ngram_range=(1,3))\n",
    "X=cv.fit_transform(new_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(cv.vocabulary_.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Most frequently occuring words\n",
    "def get_top_n_words(corpus, n=None):\n",
    "    vec = CountVectorizer().fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in      \n",
    "                   vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
    "                       reverse=True)\n",
    "    return words_freq[:n]\n",
    "#Convert most freq words to dataframe for plotting bar plot\n",
    "top_words = get_top_n_words(corpus, n=20)\n",
    "top_df = pd.DataFrame(top_words)\n",
    "top_df.columns=[\"Word\", \"Freq\"]\n",
    "#Barplot of most freq words\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(13,8)})\n",
    "g = sns.barplot(x=\"Word\", y=\"Freq\", data=top_df)\n",
    "g.set_xticklabels(g.get_xticklabels(), rotation=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Most frequently occuring Bi-grams\n",
    "def get_top_n2_words(corpus, n=None):\n",
    "    vec1 = CountVectorizer(ngram_range=(2,2),  \n",
    "            max_features=2000).fit(corpus)\n",
    "    bag_of_words = vec1.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in     \n",
    "                  vec1.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
    "                reverse=True)\n",
    "    return words_freq[:n]\n",
    "top2_words = get_top_n2_words(corpus, n=20)\n",
    "top2_df = pd.DataFrame(top2_words)\n",
    "top2_df.columns=[\"Bi-gram\", \"Freq\"]\n",
    "print(top2_df)\n",
    "#Barplot of most freq Bi-grams\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(13,8)})\n",
    "h=sns.barplot(x=\"Bi-gram\", y=\"Freq\", data=top2_df)\n",
    "h.set_xticklabels(h.get_xticklabels(), rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Most frequently occuring Tri-grams\n",
    "def get_top_n3_words(corpus, n=None):\n",
    "    vec1 = CountVectorizer(ngram_range=(3,3), \n",
    "           max_features=2000).fit(corpus)\n",
    "    bag_of_words = vec1.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in     \n",
    "                  vec1.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
    "                reverse=True)\n",
    "    return words_freq[:n]\n",
    "top3_words = get_top_n3_words(corpus, n=20)\n",
    "top3_df = pd.DataFrame(top3_words)\n",
    "top3_df.columns=[\"Tri-gram\", \"Freq\"]\n",
    "print(top3_df)\n",
    "#Barplot of most freq Tri-grams\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(13,8)})\n",
    "j=sns.barplot(x=\"Tri-gram\", y=\"Freq\", data=top3_df)\n",
    "j.set_xticklabels(j.get_xticklabels(), rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    " \n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(X)\n",
    "# get feature names\n",
    "feature_names=cv.get_feature_names()\n",
    " \n",
    "# fetch document for which keywords needs to be extracted\n",
    "doc=corpus[532]\n",
    " \n",
    "#generate tf-idf for the given document\n",
    "tf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for sorting tf_idf in descending order\n",
    "from scipy.sparse import coo_matrix\n",
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    " \n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    " \n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    \n",
    "    # word index and corresponding tf-idf score\n",
    "    for idx, score in sorted_items:\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    " \n",
    "    #create a tuples of feature,score\n",
    "    #results = zip(feature_vals,score_vals)\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "    return results\n",
    "#sort the tf-idf vectors by descending order of scores\n",
    "sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "#extract only the top n; n here is 10\n",
    "keywords=extract_topn_from_vector(feature_names,sorted_items,5)\n",
    " \n",
    "# now print the results\n",
    "print(\"\\nAbstract:\")\n",
    "print(doc)\n",
    "print(\"\\nKeywords:\")\n",
    "for k in keywords:\n",
    "    print(k,keywords[k])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
